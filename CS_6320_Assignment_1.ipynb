{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "704a4cd1",
      "metadata": {
        "id": "704a4cd1"
      },
      "source": [
        "# Assignment 1: N-Gram Language Models\n",
        "**Group 15:** Anil Lingala (akl180001), Kenneth Ly (kll200003), Anshuman Das (axd210123), Parisa Nawar (pxn210032)  \n",
        "**CS 6320.001**  \n",
        "**09/07/25**  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76830d26",
      "metadata": {
        "id": "76830d26"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82e0074a",
      "metadata": {
        "id": "82e0074a"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import math\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b3f2e6f",
      "metadata": {
        "id": "1b3f2e6f"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "You are given (via eLearning) a corpus of reviews of Chicago hotels Each line in these files corresponds to a single review. Here is an example:  \n",
        "\n",
        "```\n",
        "After Leaving some important documents in the room, I called and asked for the lost and\n",
        "found department... SEVEN TIMES over the course of a week. Eventually I had enough and\n",
        "asked for a manager and was put on hold then disconnected. Finally , a deceivingly friendly\n",
        "operator PROMISED me she would have someone call me back in a few minutes, It never\n",
        "happened. So Avoid this place because after they rip you off once they’ll keep doing it later.\n",
        "They cannot be trusted at their word.\n",
        "```\n",
        "**\\*Note for Google Colab you wil have to upload train.txt and val.txt to session storage every time you start a new session\\***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29fb85fc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29fb85fc",
        "outputId": "48e70a35-f5c2-45d7-820d-b331202807b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample training review: i booked two rooms four months in advance at the talbott . we were placed on the top floor next to the elevators , which are used all night long . when speaking to the front desk , i was told that they were simply honoring my request for an upper floor , which i had requested for a better view . i am looking at a brick wall , and getting no sleep . he also told me that they had received complaints before from guests on the 16th floor , and were aware of the noise problem . why then did they place us on this floor when the hotel is not totally booked ? a request for an upper floor does not constitute placing someone on the top floor and using that request to justify this . if you decide to stay here , request a room on a lower floor and away from the elevator ! i spoke at length when booking my two rooms about my preferences . this is simply poor treatment of a guest whom they believed would not complain .\n"
          ]
        }
      ],
      "source": [
        "#Open the files and read them\n",
        "trainFile = \"train.txt\"\n",
        "valFile = \"val.txt\"\n",
        "\n",
        "with open(trainFile, \"r\", encoding=\"utf-8\") as f:\n",
        "    trainCorpus = [line.lower().strip() for line in f]\n",
        "\n",
        "with open(valFile, \"r\", encoding=\"utf-8\") as f:\n",
        "    valCorpus = [line.lower().strip() for line in f]\n",
        "\n",
        "print(\"Sample training review:\", trainCorpus[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c0ff1b9",
      "metadata": {
        "id": "5c0ff1b9"
      },
      "source": [
        "# Preprocessing Decisions and Helper Functions\n",
        "\n",
        "\n",
        "Preprocessing The files included are already tokenized and hence it should be straightforward to obtain the tokens by using space as the delimiter. Feel free to do any other preprocessing that you might think is important for this corpus. Do not forget to describe and explain your pre-processing choices in your report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2f16a84",
      "metadata": {
        "id": "b2f16a84"
      },
      "outputs": [],
      "source": [
        "#Clean each train review and split it individually\n",
        "def cleanLines(corpus):\n",
        "  cleanedLines = []\n",
        "  for line in corpus:\n",
        "    line = re.sub(r'[^\\w\\s]', '', line)\n",
        "    line = line.split(' ')\n",
        "\n",
        "    while '' in line:\n",
        "          line.remove('')\n",
        "\n",
        "    while '\\n' in line:\n",
        "          line.remove('\\n')\n",
        "\n",
        "    line.insert(0, '<s>')\n",
        "    line.append('</stop>')\n",
        "    cleanedLines.append(line)\n",
        "  return cleanedLines\n",
        "\n",
        "\n",
        "#Helper function to sort dictionaries for printing\n",
        "def sort(dict):\n",
        "  return sorted(dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "#Helper function to ensure that  the calculated probabilites of the models add up to 1\n",
        "def validateProbabilties(dict):\n",
        "  total_prob = sum(list(dict.values()))\n",
        "  print(\"\\nTotal probability:\", total_prob)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleanTrainLines = cleanLines(trainCorpus)\n",
        "cleanTestLines = cleanLines(valCorpus)\n",
        "print(\"trainLines: \", cleanTrainLines[0])\n",
        "print(\"testLines: \", cleanTestLines[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWGZDB6_kqsS",
        "outputId": "b01462d8-b172-4035-a9a4-c582ceffd4cc"
      },
      "id": "pWGZDB6_kqsS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainLines:  ['<s>', 'i', 'booked', 'two', 'rooms', 'four', 'months', 'in', 'advance', 'at', 'the', 'talbott', 'we', 'were', 'placed', 'on', 'the', 'top', 'floor', 'next', 'to', 'the', 'elevators', 'which', 'are', 'used', 'all', 'night', 'long', 'when', 'speaking', 'to', 'the', 'front', 'desk', 'i', 'was', 'told', 'that', 'they', 'were', 'simply', 'honoring', 'my', 'request', 'for', 'an', 'upper', 'floor', 'which', 'i', 'had', 'requested', 'for', 'a', 'better', 'view', 'i', 'am', 'looking', 'at', 'a', 'brick', 'wall', 'and', 'getting', 'no', 'sleep', 'he', 'also', 'told', 'me', 'that', 'they', 'had', 'received', 'complaints', 'before', 'from', 'guests', 'on', 'the', '16th', 'floor', 'and', 'were', 'aware', 'of', 'the', 'noise', 'problem', 'why', 'then', 'did', 'they', 'place', 'us', 'on', 'this', 'floor', 'when', 'the', 'hotel', 'is', 'not', 'totally', 'booked', 'a', 'request', 'for', 'an', 'upper', 'floor', 'does', 'not', 'constitute', 'placing', 'someone', 'on', 'the', 'top', 'floor', 'and', 'using', 'that', 'request', 'to', 'justify', 'this', 'if', 'you', 'decide', 'to', 'stay', 'here', 'request', 'a', 'room', 'on', 'a', 'lower', 'floor', 'and', 'away', 'from', 'the', 'elevator', 'i', 'spoke', 'at', 'length', 'when', 'booking', 'my', 'two', 'rooms', 'about', 'my', 'preferences', 'this', 'is', 'simply', 'poor', 'treatment', 'of', 'a', 'guest', 'whom', 'they', 'believed', 'would', 'not', 'complain', '</stop>']\n",
            "testLines:  ['<s>', 'i', 'stayed', 'for', 'four', 'nights', 'while', 'attending', 'a', 'conference', 'the', 'hotel', 'is', 'in', 'a', 'great', 'spot', 'easy', 'walk', 'to', 'michigan', 'ave', 'shopping', 'or', 'rush', 'st', 'but', 'just', 'off', 'the', 'busy', 'streets', 'the', 'room', 'i', 'had', 'was', 'spacious', 'and', 'very', 'wellappointed', 'the', 'staff', 'was', 'friendly', 'and', 'the', 'fitness', 'center', 'while', 'not', 'huge', 'was', 'wellequipped', 'and', 'clean', 'i', 've', 'stayed', 'at', 'a', 'number', 'of', 'hotels', 'in', 'chicago', 'and', 'this', 'one', 'is', 'my', 'favorite', 'internet', 'was', 'nt', 'free', 'but', 'at', '10', 'for', '24', 'hours', 'is', 'cheaper', 'than', 'most', 'business', 'hotels', 'and', 'it', 'worked', 'very', 'well', '</stop>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c03f030e",
      "metadata": {
        "id": "c03f030e"
      },
      "source": [
        "# Unsmoothed Unigram Model\n",
        "\n",
        "You must write the code for gathering n-gram counts and computing n-gram probabilities yourself. For example, consider the simple corpus consisting of the sole sentence:  \n",
        "\n",
        "```\n",
        "the students like the assignment\n",
        "```\n",
        "Part of what your program would compute for a unigram and bigram model, for example, would be the following:  \n",
        "\n",
        "```\n",
        "P (the) = 0.4, P (like) = 0.2\n",
        "P (the|like) = 1.0, P (students|the) = 0.5\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create word dictionary and count total words for unigram\n",
        "def countUnigrams(corpus):\n",
        "  unigramCount = {}\n",
        "  for line in corpus:\n",
        "      for word in line:\n",
        "          if word not in unigramCount:\n",
        "              unigramCount[word] = 1\n",
        "          else:\n",
        "              unigramCount[word] += 1\n",
        "  return unigramCount\n",
        "\n",
        "baseUnigramCount = countUnigrams(cleanTrainLines)\n",
        "totalUnigrams = sum(baseUnigramCount.values())\n",
        "# print(\"Start unigram: \", baseUnigramCount[\"<s>\"])\n",
        "# print(\"Stop unigram: \", baseUnigramCount[\"</stop>\"])\n",
        "print(\"Top 5 unigram counts: \",sort(baseUnigramCount)[:5])\n",
        "print(\"Total unigrams in corpus: \",totalUnigrams)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcDNU1e3f1xE",
        "outputId": "22499a9e-285d-4124-c200-f16b1a26fb6f"
      },
      "id": "qcDNU1e3f1xE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 unigram counts:  [('the', 5302), ('and', 2593), ('a', 2247), ('to', 2090), ('was', 1826)]\n",
            "Total unigrams in corpus:  80423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate unigram probabilities on training data\n",
        "unigramModel = {}\n",
        "for word, count in baseUnigramCount.items():\n",
        "    probability = count / totalUnigrams\n",
        "    unigramModel[word] = probability\n",
        "\n",
        "print(\"Top 5 unigram probabilities: \",sort(unigramModel)[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCN9fu-Cvjsc",
        "outputId": "f189b8a7-14ec-4c28-d21b-9e3492dac481"
      },
      "id": "HCN9fu-Cvjsc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 unigram probabilities:  [('the', 0.0659264140855228), ('and', 0.03224202031757084), ('a', 0.027939768474192706), ('to', 0.025987590614625168), ('was', 0.022704947589619884)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsmoothed Bigram Model"
      ],
      "metadata": {
        "id": "A5oUBGMw-y-p"
      },
      "id": "A5oUBGMw-y-p"
    },
    {
      "cell_type": "code",
      "source": [
        "#Create word dictionary and count total words for bigram\n",
        "\n",
        "from operator import itemgetter\n",
        "\n",
        "def countBigrams(corpus):\n",
        "  bigramCount = {}\n",
        "  for line in corpus:\n",
        "      for i in range(len(line) - 1):\n",
        "          if (line[i], line[i + 1]) not in bigramCount:\n",
        "              bigramCount[(line[i], line[i + 1])] = 1\n",
        "          else:\n",
        "              bigramCount[(line[i], line[i + 1])] += 1\n",
        "  return bigramCount\n",
        "\n",
        "baseBigramCount = countBigrams(cleanTrainLines)\n",
        "totalBigrams = sum(baseBigramCount.values())\n",
        "\n",
        "# print(\"Bigrams starting with <s>: \", sorted([(item[0], item[1]) for item in list(baseBigramCount.items()) if \"<s>\" in item[0]], key = itemgetter(1), reverse=True))\n",
        "# print(\"Bigrams starting with </stop>: \", sorted([(item[0], item[1]) for item in list(baseBigramCount.items()) if \"</stop>\" in item[0]], key = itemgetter(1), reverse=True))\n",
        "print(\"Top 5 bigram counts: \", sort(baseBigramCount)[:5])\n",
        "print(\"Total bigrams in corpus: \", totalBigrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMHDtAHuuX_b",
        "outputId": "5b4ade1f-ac6d-403f-b44d-499d59d3fe22"
      },
      "id": "rMHDtAHuuX_b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 bigram counts:  [(('in', 'the'), 420), (('the', 'hotel'), 414), (('of', 'the'), 343), (('at', 'the'), 335), (('the', 'room'), 295)]\n",
            "Total bigrams in corpus:  79911\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate bigram probabilities\n",
        "bigramModel = {}\n",
        "\n",
        "for word, count in baseBigramCount.items():\n",
        "    probability = count / (baseUnigramCount[word[0]])\n",
        "    bigramModel[word] = probability\n",
        "\n",
        "print(\"bigram probabilities: \", list(bigramModel.items())[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzz4dfXFuvJM",
        "outputId": "4afcd3f7-ab80-4fc5-f748-c306bdb08e62"
      },
      "id": "fzz4dfXFuvJM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigram probabilities:  [(('<s>', 'i'), 0.216796875), (('i', 'booked'), 0.012273524254821741), (('booked', 'two'), 0.011627906976744186), (('two', 'rooms'), 0.0234375), (('rooms', 'four'), 0.0049504950495049506)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ed6b819",
      "metadata": {
        "id": "9ed6b819"
      },
      "source": [
        "# Smoothing and Unknown Words\n",
        "\n",
        "Firstly, you should implement one or more than one methods to handle unknown words. Then You will need to implement two smoothing methods (e.g. Laplace, Add-k smoothing with different k). Teams can choose any method(s) that they want for each. The report should make clear what methods were selected, providing a description for any non-standard approach (e.g., an approach that was not covered in class or in the class)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unknown Words"
      ],
      "metadata": {
        "id": "dFeDEU_nlvMw"
      },
      "id": "dFeDEU_nlvMw"
    },
    {
      "cell_type": "code",
      "source": [
        "#Replaces every word that only appears n times in corpus with <unk>\n",
        "def unknownWords(corpus, n):\n",
        "  unkCorpus = copy.deepcopy(corpus)\n",
        "  for line in unkCorpus:\n",
        "      for i in range(len(line)):\n",
        "          tok = line[i]\n",
        "          if baseUnigramCount.get(line[i], 0) <= n:\n",
        "              line[i] = '<unk>'\n",
        "  return unkCorpus\n"
      ],
      "metadata": {
        "id": "G8MhQZCuUxvL"
      },
      "id": "G8MhQZCuUxvL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unkTrainLines = unknownWords(cleanTrainLines, 1)\n",
        "\n",
        "unkUnigramCount = countUnigrams(unkTrainLines)\n",
        "print(\"Top 5 unigram counts: \",sort(unkUnigramCount)[:5])\n",
        "\n",
        "unkBigramCount = countBigrams(unkTrainLines)\n",
        "print(\"Top 5 bigram counts: \", sort(unkBigramCount)[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BClVgfCdroJu",
        "outputId": "f6ddf507-7b87-41bc-ff10-2f234749b9c1"
      },
      "id": "BClVgfCdroJu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 unigram counts:  [('the', 5302), ('<unk>', 3113), ('and', 2593), ('a', 2247), ('to', 2090)]\n",
            "Top 5 bigram counts:  [(('in', 'the'), 420), (('the', 'hotel'), 414), (('of', 'the'), 343), (('at', 'the'), 335), (('the', 'room'), 295)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Smoothing"
      ],
      "metadata": {
        "id": "wNHSTcMBDyx-"
      },
      "id": "wNHSTcMBDyx-"
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement unigram smoothing method\n",
        "def unigramSmoothing(ngramCounts, totalNgrams, vocabSize, k):\n",
        "    smoothedModel = {}\n",
        "    for ngram, count in ngramCounts.items():\n",
        "        prob = (count + k) / (totalNgrams + (k * vocabSize))\n",
        "        smoothedModel[ngram] = prob\n",
        "    return smoothedModel\n",
        "\n",
        "# Implement bigram smoothing method\n",
        "def bigramSmoothing(ngramCounts, totalNgrams, vocabSize, k):\n",
        "    smoothedModel = {}\n",
        "    for ngram, count in ngramCounts.items():\n",
        "        prob = (count + k) / (unkUnigramCount[ngram[0]] + (k * vocabSize))\n",
        "        smoothedModel[ngram] = prob\n",
        "    return smoothedModel"
      ],
      "metadata": {
        "id": "sXVAp_4AEMB5"
      },
      "id": "sXVAp_4AEMB5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unigram Smoothing"
      ],
      "metadata": {
        "id": "UBymjXRuKXRd"
      },
      "id": "UBymjXRuKXRd"
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Laplace and add-K smoothing for unigram model\n",
        "laplaceSmoothedUnigramModel = unigramSmoothing(unkUnigramCount, totalUnigrams, len(unkUnigramCount), 1)\n",
        "k5SmoothedUnigramModel = unigramSmoothing(unkUnigramCount, totalUnigrams, len(unkUnigramCount), 5)\n",
        "\n",
        "print(\"Laplace Smoothed Unigram Model:\", sort(laplaceSmoothedUnigramModel)[:5])\n",
        "print(\"K=5 Smoothed Unigram Model:\", sort(k5SmoothedUnigramModel)[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y13IiUlTKf9M",
        "outputId": "a8f2125f-dc0f-4335-c070-6e19d62ad252"
      },
      "id": "Y13IiUlTKf9M",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Laplace Smoothed Unigram Model: [('the', 0.06350746089914014), ('<unk>', 0.037292519939642166), ('and', 0.031065124188642188), ('a', 0.02692151086201528), ('to', 0.025041316375655674)]\n",
            "K=5 Smoothed Unigram Model: [('the', 0.0553862531048446), ('<unk>', 0.03254085871130685), ('and', 0.027113903441942016), ('a', 0.02350289089732618), ('to', 0.021864367864075644)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bigram Smoothing"
      ],
      "metadata": {
        "id": "sau210FrI5yh"
      },
      "id": "sau210FrI5yh"
    },
    {
      "cell_type": "code",
      "source": [
        "#Include the 0-count n-grams in the bigramCount word dictionary\n",
        "words = list(unkUnigramCount.keys())\n",
        "print(\"Total Unique Bigrams without 0-count: \", len(unkBigramCount))\n",
        "\n",
        "for i in range(len(words)):\n",
        "    for j in range(len(words)):\n",
        "        pair = (words[i], words[j])\n",
        "        if pair not in unkBigramCount:\n",
        "          unkBigramCount[pair] = 0\n",
        "\n",
        "print(\"Total Unique Bigrams with 0-count: \", len(unkBigramCount))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04InFp5IKQbK",
        "outputId": "0dffd8b6-481c-4ecb-e04f-222051add24b"
      },
      "id": "04InFp5IKQbK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Unique Bigrams without 0-count:  33266\n",
            "Total Unique Bigrams with 0-count:  9480241\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccd6b032",
      "metadata": {
        "id": "ccd6b032",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1228519c-07b1-4b39-da3e-bb4726fac82a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Laplace Smoothed Bigram Model:  [(('in', 'the'), 0.09702696473841899), (('at', 'the'), 0.08786610878661087), (('of', 'the'), 0.0833939393939394), (('on', 'the'), 0.06265125033611185), (('this', 'hotel'), 0.05717397222978492)]\n",
            "K=5 Smoothed Bigram Model:  [(('in', 'the'), 0.025517862503752625), (('of', 'the'), 0.021166595705857306), (('at', 'the'), 0.021065675340768277), (('the', 'hotel'), 0.020244479876310575), (('and', 'the'), 0.015899488547920837)]\n"
          ]
        }
      ],
      "source": [
        "#Apply Laplace and add-K smoothing for bigram model\n",
        "laplaceSmoothedBigramModel = bigramSmoothing(unkBigramCount, totalBigrams, len(unkUnigramCount), 1)\n",
        "k5SmoothedBigramModel = bigramSmoothing(unkBigramCount, totalBigrams, len(unkUnigramCount), 5)\n",
        "\n",
        "print(\"Laplace Smoothed Bigram Model: \", sort(laplaceSmoothedBigramModel)[:5])\n",
        "print(\"K=5 Smoothed Bigram Model: \", sort(k5SmoothedBigramModel)[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62f69dc6",
      "metadata": {
        "id": "62f69dc6"
      },
      "source": [
        "## Perplexity Calculation  \n",
        "\n",
        "Implement code to compute the perplexity of a “development set.” (“development set” is just another way to refer to the validation set — part of a dataset that is distinct from the training portion.) Compute and report the perplexity of your model (with variations) on it. Under the second definition above, perplexity is a function of the average (per-word) log probability: use this to avoid numerical computation errors.  \n",
        "\n",
        "If you experimented with more than one type of smoothing and unknown word handling, you should report and compare the perplexity results of experiments among some of them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70c3c010",
      "metadata": {
        "id": "70c3c010"
      },
      "outputs": [],
      "source": [
        "# Function to compute unigram perplexity\n",
        "def unigramPerplexity(valCleanLines, unigramCounts, k):\n",
        "  wordCount, log_sum=0, 0.0\n",
        "  vSize = len(unigramCounts)\n",
        "  unkUniSum = sum(unigramCounts.values())\n",
        "\n",
        "  for read in valCleanLines:\n",
        "    for currentWord in read:\n",
        "      if currentWord in ('<s>', '</stop>'):\n",
        "          continue\n",
        "\n",
        "      curr = unigramCounts.get(currentWord, 0)\n",
        "      prob = (curr + k) / (unkUniSum + k * vSize)\n",
        "      log_sum += math.log(prob)\n",
        "      wordCount += 1\n",
        "\n",
        "  if wordCount == 0:\n",
        "        raise ValueError(\"No valid unigram token found after skipping.\")\n",
        "  return math.exp(-log_sum/wordCount)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute bigram perplexity\n",
        "def bigramPerplexity(valCleanLines, unigramCounts, bigramCounts, k):\n",
        "  wordCount, log_sum=0, 0.0\n",
        "  vSize = len(unigramCounts)\n",
        "  unkBigSum = sum(bigramCounts.values())\n",
        "\n",
        "  for read in valCleanLines:\n",
        "    for i in range(1, len(read)):\n",
        "      prevWord = read[i-1]\n",
        "      currentWord = read[i]\n",
        "      if prevWord in ('<s>', '</stop>') or currentWord in ('<s>', '</stop>'):\n",
        "          continue\n",
        "\n",
        "      prevC = unigramCounts.get(prevWord, 0)\n",
        "      bigCount = bigramCounts.get((prevWord, currentWord), 0)\n",
        "\n",
        "      denominator = prevC + k * vSize\n",
        "      p = (bigCount + k) / (denominator if denominator > 0 else k * vSize)\n",
        "      log_sum += math.log(p)\n",
        "      wordCount += 1\n",
        "\n",
        "  if wordCount == 0:\n",
        "        raise ValueError(\"No valid bigram token found after skipping.\")\n",
        "  return math.exp(-log_sum/wordCount)"
      ],
      "metadata": {
        "id": "BhGjN8Df9knI"
      },
      "id": "BhGjN8Df9knI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unkTrainLines = unknownWords(cleanTrainLines, 1)\n",
        "unkTestLines  = unknownWords(cleanTestLines, 1)\n",
        "unkUnigramCount = countUnigrams(unkTrainLines)\n",
        "unkBigramCount  = countBigrams(unkTrainLines)\n",
        "\n",
        "# Unigram perplexities\n",
        "print(\"Unigram Perplexity (Laplace k=1):\", unigramPerplexity(unkTestLines, unkUnigramCount, 1.0))\n",
        "\n",
        "print(\"Unigram Perplexity (Add-k k=5):\", unigramPerplexity(unkTestLines, unkUnigramCount, 5.0))\n",
        "\n",
        "# Bigram perplexities\n",
        "print(\"Bigram Perplexity (Laplace k=1):\", bigramPerplexity(unkTestLines, unkUnigramCount, unkBigramCount, 1.0))\n",
        "\n",
        "print(\"Bigram Perplexity (Add-k k=5):\", bigramPerplexity(unkTestLines, unkUnigramCount, unkBigramCount, 5.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fq41IC3p632R",
        "outputId": "0aa9605c-ac5d-4ea4-dc97-3f7c33123e13"
      },
      "id": "Fq41IC3p632R",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Perplexity (Laplace k=1): 290.73172953869454\n",
            "Unigram Perplexity (Add-k k=5): 299.5746749358359\n",
            "Bigram Perplexity (Laplace k=1): 374.6149788575254\n",
            "Bigram Perplexity (Add-k k=5): 736.996281527752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unigram perplexities with training set\n",
        "print(\"Unigram Perplexity Training (Unsmoothed k=0):\",\n",
        "      unigramPerplexity(unkTrainLines, unkUnigramCount, 0.0))\n",
        "\n",
        "print(\"Unigram Perplexity Training (Laplace k=1):\",\n",
        "      unigramPerplexity(unkTrainLines, unkUnigramCount, 1.0))\n",
        "\n",
        "print(\"Unigram Perplexity Training (Add-k k=5):\",\n",
        "      unigramPerplexity(unkTrainLines, unkUnigramCount, 5.0))\n",
        "\n",
        "print(\"Unigram Perplexity Training (Add-k k=0.01):\",\n",
        "      unigramPerplexity(unkTrainLines, unkUnigramCount, 0.01))\n",
        "\n",
        "\n",
        "# Unigram perplexities with validation set\n",
        "print(\"Unigram Perplexity (Unsmoothed k=0):\",\n",
        "      unigramPerplexity(unkTestLines, unkUnigramCount, 0.0))\n",
        "\n",
        "print(\"Unigram Perplexity (Laplace k=1):\",\n",
        "      unigramPerplexity(unkTestLines, unkUnigramCount, 1.0))\n",
        "\n",
        "print(\"Unigram Perplexity (Add-k k=5):\",\n",
        "      unigramPerplexity(unkTestLines, unkUnigramCount, 5))\n",
        "\n",
        "print(\"Unigram Perplexity (Add-k k=0.01):\",\n",
        "      unigramPerplexity(unkTestLines, unkUnigramCount, 0.01))\n",
        "\n",
        "# Bigram perplexities\n",
        "print(\"Bigram Perplexity Training (Laplace k=1):\",\n",
        "      bigramPerplexity(unkTrainLines, unkUnigramCount, unkBigramCount, 1.0))\n",
        "\n",
        "print(\"Bigram Perplexity Training (Add-k k=5):\",\n",
        "      bigramPerplexity(unkTrainLines, unkUnigramCount, unkBigramCount, 5.0))\n",
        "\n",
        "print(\"Bigram Perplexity Training (Add-k k=0.01):\",\n",
        "      bigramPerplexity(unkTrainLines, unkUnigramCount, unkBigramCount, 0.01))\n",
        "\n",
        "print(\"Bigram Perplexity (Laplace k=1):\",\n",
        "      bigramPerplexity(unkTestLines, unkUnigramCount, unkBigramCount, 1.0))\n",
        "\n",
        "print(\"Bigram Perplexity (Add-k k=5):\",\n",
        "      bigramPerplexity(unkTestLines, unkUnigramCount, unkBigramCount, 5.0))\n",
        "\n",
        "print(\"Bigram Perplexity (Add-k k=0.01):\",\n",
        "      bigramPerplexity(unkTestLines, unkUnigramCount, unkBigramCount, 0.01))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-1aLqX9zqMo",
        "outputId": "7542dbea-ed3b-42d8-b2b3-231fedfa54c9"
      },
      "id": "a-1aLqX9zqMo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Perplexity Training (Unsmoothed k=0): 386.11772158789967\n",
            "Unigram Perplexity Training (Laplace k=1): 387.20980605600755\n",
            "Unigram Perplexity Training (Add-k k=5): 402.5512281420322\n",
            "Unigram Perplexity Training (Add-k k=0.01): 386.11607557700347\n",
            "Unigram Perplexity (Unsmoothed k=0): 335.9370345355836\n",
            "Unigram Perplexity (Laplace k=1): 338.7385266431256\n",
            "Unigram Perplexity (Add-k k=5): 356.908884001531\n",
            "Unigram Perplexity (Add-k k=0.01): 335.95702789316056\n",
            "Bigram Perplexity Training (Laplace k=1): 459.00977621500067\n",
            "Bigram Perplexity Training (Add-k k=5): 1077.4830558111491\n",
            "Bigram Perplexity Training (Add-k k=0.01): 52.39193778150466\n",
            "Bigram Perplexity (Laplace k=1): 543.6087164017083\n",
            "Bigram Perplexity (Add-k k=5): 1095.2674933291883\n",
            "Bigram Perplexity (Add-k k=0.01): 190.58533397139834\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}